{
  "cells": [   
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEixZDy2SAxf"
      },
      "source": [
        "# Preparing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgOShdW807u0"
      },
      "source": [
        "## pip installs and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcvC-6XzAT-z"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJZ94pW107u3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU18_4go07u4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGBVKtsL07u4"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiurqUoNAoIz"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FMpmgnrcGiw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import trange\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "from transformers import AutoModel, ElectraTokenizer\n",
        "from collections import Counter\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55qzb-s6ushs"
      },
      "source": [
        "## git clone and file download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj_6AcAVuwJ3",
        "outputId": "56355cba-59cc-400d-e8cd-0018f3a66bfc"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/HappyBusDay/Korean_ABSA.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1aWOyqC4nMD"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urIlJHaHcO4A"
      },
      "outputs": [],
      "source": [
        "PADDING_TOKEN = 1\n",
        "S_OPEN_TOKEN = 0\n",
        "S_CLOSE_TOKEN = 2\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# pretrained_models\n",
        "model_ELECTRA = 'kykim/electra-kor-base'\n",
        "model_RoBERTa = 'xlm-roberta-base'\n",
        "model_DeBERTa = \"lighthouse/mdeberta-v3-base-kor-further\"\n",
        "\n",
        "entity_property_pair = [    # 카테고리의 수 = 25개\n",
        "     '패키지/구성품#다양성','본품#인지도','브랜드#디자인',\n",
        "     '패키지/구성품#편의성','제품 전체#디자인', '제품 전체#품질',\n",
        "     '패키지/구성품#품질','패키지/구성품#일반','본품#일반',\n",
        "     '패키지/구성품#디자인','본품#편의성','브랜드#품질',\n",
        "     '브랜드#인지도','본품#다양성','본품#디자인',\n",
        "     '제품 전체#다양성','본품#품질','제품 전체#인지도',\n",
        "     '패키지/구성품#가격','본품#가격','제품 전체#가격',\n",
        "     '브랜드#가격','브랜드#일반','제품 전체#일반','제품 전체#편의성'\n",
        "     ]\n",
        "\n",
        "tf_id_to_name = ['True', 'False']\n",
        "tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n",
        "\n",
        "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
        "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiVfH_4xCw_Y"
      },
      "source": [
        "# functions and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JUSPOfy4phz"
      },
      "source": [
        "## (func) jsonload, jsondump, jsonlload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJv-Zl-I4riC"
      },
      "outputs": [],
      "source": [
        "def jsonload(fname, encoding=\"utf-8\"):\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        j = json.load(f)\n",
        "\n",
        "    return j\n",
        "\n",
        "# json 개체를 파일이름으로 깔끔하게 저장\n",
        "def jsondump(j, fname):\n",
        "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
        "        json.dump(j, f, ensure_ascii=False)\n",
        "\n",
        "# jsonl 파일 읽어서 list에 저장\n",
        "def jsonlload(fname, encoding=\"utf-8\"):\n",
        "    json_list = []\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        for line in f.readlines():\n",
        "            json_list.append(json.loads(line))\n",
        "    return json_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVaEgV7T_88W"
      },
      "source": [
        "## (func) tokenize_and_align_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT6IfwXK_7iR"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
        "\n",
        "    entity_property_data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': []\n",
        "    }\n",
        "    polarity_data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    for pair in entity_property_pair:\n",
        "        isPairInOpinion = False\n",
        "        if pd.isna(form):\n",
        "            break\n",
        "        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n",
        "        for annotation in annotations:\n",
        "            entity_property = annotation[0]\n",
        "            polarity = annotation[2]\n",
        "\n",
        "            if polarity == '------------':\n",
        "                continue\n",
        "\n",
        "            if entity_property == pair:\n",
        "                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n",
        "\n",
        "                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n",
        "\n",
        "                isPairInOpinion = True\n",
        "                break\n",
        "\n",
        "        if isPairInOpinion is False:\n",
        "            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n",
        "\n",
        "    return entity_property_data_dict, polarity_data_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLQn3LxWABNd"
      },
      "source": [
        "## (func) get_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ocqrb6QjACH_"
      },
      "outputs": [],
      "source": [
        "def get_dataset(raw_data, tokenizer, max_len):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    token_labels_list = []\n",
        "\n",
        "    polarity_input_ids_list = []\n",
        "    polarity_attention_mask_list = []\n",
        "    polarity_token_labels_list = []\n",
        "\n",
        "    for utterance in raw_data:\n",
        "        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n",
        "        \n",
        "        input_ids_list.extend(entity_property_data_dict['input_ids'])\n",
        "        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n",
        "        token_labels_list.extend(entity_property_data_dict['label'])\n",
        "\n",
        "        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n",
        "        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n",
        "        polarity_token_labels_list.extend(polarity_data_dict['label'])\n",
        "\n",
        "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
        "                         torch.tensor(token_labels_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n",
        "                         torch.tensor(polarity_token_labels_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-xisSs07u9"
      },
      "source": [
        "## (class) SimpleClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6cjejoy07u9"
      },
      "outputs": [],
      "source": [
        "# baseline\n",
        "class SimpleClassifier_Base(nn.Module):\n",
        "    def __init__(self, num_label, classifier_hidden):\n",
        "        super().__init__()\n",
        "\n",
        "        if layer_use == True:\n",
        "            self.dense1 = nn.Linear(classifier_hidden, classifier_hidden//2)\n",
        "            self.dense2 = nn.Linear(classifier_hidden//2, classifier_hidden//4)\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.output = nn.Linear(classifier_hidden//4, num_label)\n",
        "        else:\n",
        "            self.dense = nn.Linear(classifier_hidden, classifier_hidden)\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.output = nn.Linear(classifier_hidden, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        if layer_use == True:\n",
        "            x = features[:, 0, :]\n",
        "            x = self.dropout(x) # layer 1\n",
        "            x = self.dense1(x)\n",
        "            x = self.dropout(x) # layer 2\n",
        "            x = self.dense2(x)\n",
        "            x = torch.tanh(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.output(x)\n",
        "            return x\n",
        "\n",
        "        else:\n",
        "            x = features[:, 0, :]\n",
        "            x = self.dropout(x)\n",
        "            x = self.dense(x)\n",
        "            x = torch.tanh(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.output(x)\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3vXCZTXxlQF"
      },
      "source": [
        "## (class) BaseClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAYwrlwFeH1s"
      },
      "source": [
        "* ELECTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5zjxEHVeOvg"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier_ELECTRA(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer, hidden_size):\n",
        "        super(BaseClassifier_ELECTRA, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained( model_ELECTRA )\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label, hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pffiJkXleP2L"
      },
      "source": [
        "* DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGFm7RuqeRdX"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier_DeBERTa(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer, hidden_size):\n",
        "        super(BaseClassifier_DeBERTa, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.deberta = AutoModel.from_pretrained( model_DeBERTa )\n",
        "        self.deberta.resize_token_embeddings(len_tokenizer)\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label, hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.deberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JUsk646eTDI"
      },
      "source": [
        "* RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSVeFQfY07vB"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier_RoBERTa(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer, hidden_size):\n",
        "        super(BaseClassifier_RoBERTa, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained( model_RoBERTa )\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label, hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsDitPTgeg1W"
      },
      "source": [
        "* ELECTRA (polarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THoLuZaiVTZG"
      },
      "outputs": [],
      "source": [
        "class BaseClassifier_Pola(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer, hidden_size):\n",
        "        super(BaseClassifier_Pola, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained( model_ELECTRA )\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label, hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnl3uK3lYd5_"
      },
      "source": [
        "## (func) predict_from_korean_form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HaNuMqrYh3R"
      },
      "outputs": [],
      "source": [
        "def predict_from_korean_form(tokenizer_cate, tokenizer_pola, ce_model, pc_model, data):\n",
        "    category_tokenizer = tokenizer_cate\n",
        "    polarity_tokenizer = tokenizer_pola\n",
        "\n",
        "    ce_model.to(device)\n",
        "    ce_model.eval()\n",
        "\n",
        "    for idx, sentence in enumerate(data):\n",
        "        form = sentence['sentence_form']\n",
        "        sentence['annotation'] = []\n",
        "        if type(form) != str:\n",
        "            print(\"form type is arong: \", form)\n",
        "            continue\n",
        "            \n",
        "        tmp= []\n",
        "        force_flag = False\n",
        "\n",
        "        # categoty(속성 범주)와 polarity(감성 범주) 모델을 각각 처리\n",
        "        for pair in entity_property_pair: \n",
        "            tokenized_data_cate = category_tokenizer(form, pair, padding='max_length', max_length = max_length, truncation=True)\n",
        "            tokenized_data_pola = polarity_tokenizer(form, pair, padding='max_length', max_length = polarity_max_length, truncation=True)\n",
        "            input_ids_cate = torch.tensor([tokenized_data_cate['input_ids']]).to(device)\n",
        "            input_ids_pola = torch.tensor([tokenized_data_pola['input_ids']]).to(device)\n",
        "            attention_mask_cate = torch.tensor([tokenized_data_cate['attention_mask']]).to(device)\n",
        "            attention_mask_pola = torch.tensor([tokenized_data_pola['attention_mask']]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids_cate, attention_mask_cate)\n",
        "                tmp.append( ce_logits[0][0] )\n",
        "            \n",
        "            # threshold 조정 시 ( default = 0 )\n",
        "            if ce_logits[0][0] > threshold: \n",
        "                ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
        "                ce_result = tf_id_to_name[ce_predictions[0]]\n",
        "\n",
        "                if ce_result == 'True':\n",
        "                    force_flag = True\n",
        "                    with torch.no_grad():\n",
        "                        _, pc_logits = pc_model(input_ids_pola, attention_mask_pola)\n",
        "\n",
        "                    pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                    pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "                    sentence['annotation'].append([pair, pc_result])\n",
        "\n",
        "        # force evaluation of argument 사용 시 (default = False)\n",
        "        if force_use == True: \n",
        "            if force_flag == False:\n",
        "                tmp = torch.tensor(tmp)\n",
        "                pair = entity_property_pair[torch.argmax(tmp)]\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids_pola, attention_mask_pola)\n",
        "\n",
        "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "                sentence['annotation'].append([pair, pc_result])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFiEby13Ya9T"
      },
      "source": [
        "## (func) test_tentiment_analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8XTJOXPT6Ey"
      },
      "outputs": [],
      "source": [
        "def test_sentiment_analysis():\n",
        "    print(\"model pt path  =\", model_pt_path)\n",
        "    print(\"polarity model pt path =\", polarity_model_pt_path)\n",
        "\n",
        "    if model_kind == 'RoBERTa':\n",
        "        tokenizer = tokenizer_roberta\n",
        "        model = BaseClassifier_RoBERTa(len(tf_id_to_name), len(tokenizer), hidden_size)\n",
        "\n",
        "    elif model_kind == 'DeBERTa':\n",
        "        tokenizer = tokenizer_deberta\n",
        "        model = BaseClassifier_DeBERTa(len(tf_id_to_name), len(tokenizer), hidden_size)\n",
        "\n",
        "    else :\n",
        "        tokenizer = tokenizer_electra\n",
        "        model = BaseClassifier_ELECTRA(len(tf_id_to_name), len(tokenizer), hidden_size)\n",
        "        \n",
        "    model.load_state_dict(torch.load(model_pt_path , map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    polarity_model = BaseClassifier_Pola(len(polarity_id_to_name), len(tokenizer_electra), polarity_hidden_size)\n",
        "    polarity_model.load_state_dict(torch.load(polarity_model_pt_path , map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form(tokenizer, tokenizer_electra, model, polarity_model, copy.deepcopy(test_data))\n",
        "\n",
        "    df_pred = pd.DataFrame(pred_data)\n",
        "\n",
        "    with open(save_path  + '.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred) ):\n",
        "            tmp = str(df_pred['annotation'][i]).replace(\"\\'\", \"\\\"\").replace('None', 'null')\n",
        "            file.write(  '{'+'\\\"id\\\": \\\"nikluge-sa-2022-test-{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                .format( str(i+1).zfill(5)  ,   df_pred['sentence_form'][i], tmp ) +'}' ) \n",
        "            file.write(\"\\n\")\n",
        "\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQjCES7DC95E"
      },
      "source": [
        "# Starting Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO8QhhHLUz2m"
      },
      "source": [
        "parameter 설명\n",
        "* model_kind (대소문자 구별)\n",
        "    - RoBERTa : RoBERTa pretrained model load\n",
        "    - DeBERTa : DeBERTa pretrained model load\n",
        "    - ELECTRA : ELECTRA pretrained model load\n",
        "* layer_use\n",
        "    - True : add 1 layer\n",
        "    - False : do not add layer (*default)\n",
        "* hidden_size\n",
        "    - defalut : 768 \n",
        "    - up sampling : 1000\n",
        "    - down sampling : 384\n",
        "* force_use\n",
        "    - True : force evalutation of argument\n",
        "    - False : do not force evaluation (*default)\n",
        "* threshold\n",
        "    - default : 0\n",
        "    - strict : 2 ~ 3\n",
        "* dropout\n",
        "    - default : 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZWTokeT18Tt"
      },
      "outputs": [],
      "source": [
        "############### 입력하세요 ###################\n",
        "\n",
        "model_kind = 'RoBERTa' \n",
        "\n",
        "layer_use = False   # default = False\n",
        "hidden_size = 768   # default = 768\n",
        "force_use = False   # default = False\n",
        "threshold = 0       # default = 0\n",
        "dropout = 0.1       # default = 0.1\n",
        "\n",
        "model_pt_path = \"./content/sample.pt\"\n",
        "polarity_model_pt_path = \"./content/sample.pt\"\n",
        "\n",
        "save_path = './test01'\n",
        "\n",
        "###########################################\n",
        "\n",
        "if model_kind == 'RoBERTa':\n",
        "    max_length = 514\n",
        "else:\n",
        "    max_length = 256\n",
        "polarity_max_length = 256\n",
        "polarity_hidden_size = 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgpSv8fJ6YeD"
      },
      "source": [
        "* 1회만 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfmKM1HsTIhM"
      },
      "outputs": [],
      "source": [
        "tokenizer_roberta = AutoTokenizer.from_pretrained( model_RoBERTa )\n",
        "tokenizer_electra = AutoTokenizer.from_pretrained( model_ELECTRA )\n",
        "tokenizer_deberta = AutoTokenizer.from_pretrained( model_DeBERTa )\n",
        "\n",
        "num_added_toks_roberta = tokenizer_roberta.add_special_tokens(special_tokens_dict)\n",
        "num_added_toks_electra = tokenizer_electra.add_special_tokens(special_tokens_dict)\n",
        "num_added_toks_deberta = tokenizer_deberta.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "test_data_path = './Korean_ABSA/data/test_data.jsonl'\n",
        "test_data = jsonlload(test_data_path)\n",
        "\n",
        "entity_property_test_data_roberta, polarity_test_data_roberta = get_dataset(test_data, tokenizer_roberta, max_length)  # max_length = 514\n",
        "entity_property_test_data_electra, polarity_test_data_electra = get_dataset(test_data, tokenizer_electra, max_length)  # max_length = 256\n",
        "entity_property_test_data_deberta, polarity_test_data_deberta = get_dataset(test_data, tokenizer_deberta, max_length)  # max_length = 256\n",
        "\n",
        "entity_property_test_dataloader_roberta = DataLoader(entity_property_test_data_roberta, shuffle=True, batch_size=batch_size)\n",
        "entity_property_test_dataloader_electra = DataLoader(entity_property_test_data_electra, shuffle=True, batch_size=batch_size)\n",
        "entity_property_test_dataloader_deberta = DataLoader(entity_property_test_data_deberta, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "polarity_test_dataloader = DataLoader(polarity_test_data_electra, shuffle=True, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw4-DqroTI2D"
      },
      "outputs": [],
      "source": [
        "test_sentiment_analysis()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LiVfH_4xCw_Y",
        "3JUSPOfy4phz",
        "hVaEgV7T_88W",
        "vLQn3LxWABNd",
        "NN-xisSs07u9",
        "_3vXCZTXxlQF",
        "Pnl3uK3lYd5_",
        "lFiEby13Ya9T"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tf27')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d8015098d1dae84219b5f36314149636e9abcaade3eb06915d5127b55dee25b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
