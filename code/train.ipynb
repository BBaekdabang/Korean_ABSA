{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgOShdW807u0"
      },
      "source": [
        "# install 하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJZ94pW107u3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU18_4go07u4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {  
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGBVKtsL07u4"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rICaNAspcMYc"
      },
      "source": [
        "# import"
      ]
    },
    { 
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FMpmgnrcGiw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import trange\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "from transformers import AutoModel, ElectraTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIL8vzIx6xd"
      },
      "source": [
        "# 모델 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXBcX7-H07u6"
      },
      "source": [
        "## 기본설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urIlJHaHcO4A"
      },
      "outputs": [],
      "source": [
        "\n",
        "PADDING_TOKEN = 1\n",
        "S_OPEN_TOKEN = 0\n",
        "S_CLOSE_TOKEN = 2\n",
        "\n",
        "do_eval=True\n",
        "\n",
        "# 모델을 학습 할 때 저장 되는 파일 경로\n",
        "category_extraction_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/category_extraction/'\n",
        "polarity_classification_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/polarity_classification/'\n",
        "\n",
        "\n",
        "# 저장된 모델 Weight 파일 경로\n",
        "test_category_extraction_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/category_extraction/category_sample.pt'\n",
        "test_polarity_classification_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/polarity_classification/polarity_sample.pt'\n",
        "\n",
        "# 데이터 파일 경로\n",
        "train_cate_data_path = '/content/drive/MyDrive/train_sample(category).jsonl'\n",
        "train_pola_data_path = '/content/drive/MyDrive/train_sample(polarity).jsonl'\n",
        "test_data_path = '/content/drive/MyDrive/test_sample.jsonl'\n",
        "\n",
        "max_len_elec = 256\n",
        "max_len_debe = 256\n",
        "max_len_robe = 514\n",
        "\n",
        "# colab pro 환경에서 RoBERTa를 돌리게 될 경우 batch_size 수정 요망 ( Out of Memory 이슈 )\n",
        "batch_size = 32\n",
        "\n",
        "#ELECTRA\n",
        "base_model_elec = 'kykim/electra-kor-base'\n",
        "#RoBERTa\n",
        "base_model_roberta = 'xlm-roberta-base'\n",
        "#DeBERTa\n",
        "base_model_deberta = \"lighthouse/mdeberta-v3-base-kor-further\"\n",
        "\n",
        "learning_rate = 3e-6\n",
        "eps = 1e-8\n",
        "num_train_epochs = 30\n",
        "\n",
        "classifier_hidden_size_base = 768\n",
        "classifier_hidden_size_down = 384   # hidden_size down\n",
        "classifier_hidden_size_up = 1000    # hidden_size up\n",
        "\n",
        "classifier_dropout_prob_base = 0.1  # dropout = 0.1\n",
        "classifier_dropout_prob_up = 0.5    # dropout = 0.5\n",
        "\n",
        "# 카테고리의 수 = 25개\n",
        "entity_property_pair = [\n",
        "     '패키지/구성품#다양성','본품#인지도','브랜드#디자인',\n",
        "     '패키지/구성품#편의성','제품 전체#디자인', '제품 전체#품질',\n",
        "     '패키지/구성품#품질','패키지/구성품#일반','본품#일반',\n",
        "     '패키지/구성품#디자인','본품#편의성','브랜드#품질',\n",
        "     '브랜드#인지도','본품#다양성','본품#디자인',\n",
        "     '제품 전체#다양성','본품#품질','제품 전체#인지도',\n",
        "     '패키지/구성품#가격','본품#가격','제품 전체#가격',\n",
        "     '브랜드#가격','브랜드#일반','제품 전체#일반','제품 전체#편의성'\n",
        "     ]\n",
        "\n",
        "tf_id_to_name = ['True', 'False']\n",
        "tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n",
        "\n",
        "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
        "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZSDDZfZr7y4"
      },
      "outputs": [],
      "source": [
        "def jsonload(fname, encoding=\"utf-8\"):\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        j = json.load(f)\n",
        "\n",
        "    return j\n",
        "\n",
        "# json 개체를 파일이름으로 깔끔하게 저장\n",
        "def jsondump(j, fname):\n",
        "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
        "        json.dump(j, f, ensure_ascii=False)\n",
        "\n",
        "# jsonl 파일 읽어서 list에 저장\n",
        "def jsonlload(fname, encoding=\"utf-8\"):\n",
        "    json_list = []\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        for line in f.readlines():\n",
        "            json_list.append(json.loads(line))\n",
        "    return json_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTQwoV4RtSRs"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
        "\n",
        "    entity_property_data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': []\n",
        "    }\n",
        "    polarity_data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    for pair in entity_property_pair:\n",
        "        isPairInOpinion = False\n",
        "        if pd.isna(form):\n",
        "            break\n",
        "        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n",
        "        for annotation in annotations:\n",
        "            entity_property = annotation[0]\n",
        "            polarity = annotation[2]\n",
        "\n",
        "            if polarity == '------------':\n",
        "                continue\n",
        "\n",
        "            if entity_property == pair:\n",
        "                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n",
        "\n",
        "                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n",
        "\n",
        "                isPairInOpinion = True\n",
        "                break\n",
        "\n",
        "        if isPairInOpinion is False:\n",
        "            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n",
        "\n",
        "    return entity_property_data_dict, polarity_data_dict\n",
        "\n",
        "\n",
        "def get_dataset(raw_data, tokenizer, max_len):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    token_labels_list = []\n",
        "\n",
        "    polarity_input_ids_list = []\n",
        "    polarity_attention_mask_list = []\n",
        "    polarity_token_labels_list = []\n",
        "\n",
        "    for utterance in raw_data:\n",
        "        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n",
        "        input_ids_list.extend(entity_property_data_dict['input_ids'])\n",
        "        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n",
        "        token_labels_list.extend(entity_property_data_dict['label'])\n",
        "\n",
        "        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n",
        "        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n",
        "        polarity_token_labels_list.extend(polarity_data_dict['label'])\n",
        "\n",
        "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
        "                         torch.tensor(token_labels_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n",
        "                         torch.tensor(polarity_token_labels_list))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ2NE8pIxdEY"
      },
      "outputs": [],
      "source": [
        "def evaluation(y_true, y_pred, label_len):\n",
        "    count_list = [0]*label_len\n",
        "    hit_list = [0]*label_len\n",
        "    for i in range(len(y_true)):\n",
        "        count_list[y_true[i]] += 1\n",
        "        if y_true[i] == y_pred[i]:\n",
        "            hit_list[y_true[i]] += 1\n",
        "    acc_list = []\n",
        "\n",
        "    for i in range(label_len):\n",
        "        acc_list.append(hit_list[i]/count_list[i])\n",
        "\n",
        "    print(count_list)\n",
        "    print(hit_list)\n",
        "    print(acc_list)\n",
        "    print('accuracy: ', (sum(hit_list) / sum(count_list)))\n",
        "    print('macro_accuracy: ', sum(acc_list) / 3)\n",
        "    # print(y_true)\n",
        "\n",
        "    y_true = list(map(int, y_true))\n",
        "    y_pred = list(map(int, y_pred))\n",
        "\n",
        "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
        "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
        "    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klOrDUrd28IM"
      },
      "outputs": [],
      "source": [
        "def evaluation_f1(true_data, pred_data):\n",
        "\n",
        "    true_data_list = true_data\n",
        "    pred_data_list = pred_data\n",
        "\n",
        "    ce_eval = {\n",
        "        'TP': 0,\n",
        "        'FP': 0,\n",
        "        'FN': 0,\n",
        "        'TN': 0\n",
        "    }\n",
        "\n",
        "    pipeline_eval = {\n",
        "        'TP': 0,\n",
        "        'FP': 0,\n",
        "        'FN': 0,\n",
        "        'TN': 0\n",
        "    }\n",
        "\n",
        "    for i in range(len(true_data_list)):\n",
        "\n",
        "        # TP, FN checking\n",
        "        is_ce_found = False\n",
        "        is_pipeline_found = False\n",
        "        for y_ano  in true_data_list[i]['annotation']:\n",
        "            y_category = y_ano[0]\n",
        "            y_polarity = y_ano[2]\n",
        "\n",
        "            for p_ano in pred_data_list[i]['annotation']:\n",
        "                p_category = p_ano[0]\n",
        "                p_polarity = p_ano[1]\n",
        "\n",
        "                if y_category == p_category:\n",
        "                    is_ce_found = True\n",
        "                    if y_polarity == p_polarity:\n",
        "                        is_pipeline_found = True\n",
        "\n",
        "                    break\n",
        "\n",
        "            if is_ce_found is True:\n",
        "                ce_eval['TP'] += 1\n",
        "            else:\n",
        "                ce_eval['FN'] += 1\n",
        "\n",
        "            if is_pipeline_found is True:\n",
        "                pipeline_eval['TP'] += 1\n",
        "            else:\n",
        "                pipeline_eval['FN'] += 1\n",
        "\n",
        "            is_ce_found = False\n",
        "            is_pipeline_found = False\n",
        "\n",
        "        # FP checking\n",
        "        for p_ano in pred_data_list[i]['annotation']:\n",
        "            p_category = p_ano[0]\n",
        "            p_polarity = p_ano[1]\n",
        "\n",
        "            for y_ano  in true_data_list[i]['annotation']:\n",
        "                y_category = y_ano[0]\n",
        "                y_polarity = y_ano[2]\n",
        "\n",
        "                if y_category == p_category:\n",
        "                    is_ce_found = True\n",
        "                    if y_polarity == p_polarity:\n",
        "                        is_pipeline_found = True\n",
        "\n",
        "                    break\n",
        "\n",
        "            if is_ce_found is False:\n",
        "                ce_eval['FP'] += 1\n",
        "\n",
        "            if is_pipeline_found is False:\n",
        "                pipeline_eval['FP'] += 1\n",
        "            is_ce_found = False\n",
        "            is_pipeline_found = False\n",
        "\n",
        "    ce_precision = ce_eval['TP']/(ce_eval['TP']+ce_eval['FP'])\n",
        "    ce_recall = ce_eval['TP']/(ce_eval['TP']+ce_eval['FN'])\n",
        "\n",
        "    ce_result = {\n",
        "        'Precision': ce_precision,\n",
        "        'Recall': ce_recall,\n",
        "        'F1': 2*ce_recall*ce_precision/(ce_recall+ce_precision)\n",
        "    }\n",
        "\n",
        "    pipeline_precision = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FP'])\n",
        "    pipeline_recall = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FN'])\n",
        "\n",
        "    pipeline_result = {\n",
        "        'Precision': pipeline_precision,\n",
        "        'Recall': pipeline_recall,\n",
        "        'F1': 2*pipeline_recall*pipeline_precision/(pipeline_recall+pipeline_precision)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'category extraction result': ce_result,\n",
        "        'entire pipeline result': pipeline_result\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-xisSs07u9"
      },
      "source": [
        "## SimpleClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6cjejoy07u9"
      },
      "outputs": [],
      "source": [
        "# baseline\n",
        "class SimpleClassifier_Base(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# hidden_size를 1000으로 up\n",
        "class SimpleClassifier_Hidden_up(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_up)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_up, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x        \n",
        "\n",
        "# hidden_size를 384로 down\n",
        "class SimpleClassifier_Hidden_down(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_down)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_down, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x        \n",
        "\n",
        "# dropout 0.5\n",
        "class SimpleClassifier_dr05(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_up)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# hidden_size를 384로 down + dropout 0.5\n",
        "class SimpleClassifier_Hidden_down_dr05(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_down)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_up)\n",
        "        self.output = nn.Linear(classifier_hidden_size_down, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x        \n",
        "\n",
        "# dense_layer를 1층 더 추가 (dropout0.1)\n",
        "class SimpleClassifier_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base//2)\n",
        "        self.dense2 = nn.Linear(classifier_hidden_size_base//2, classifier_hidden_size_base//4)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base//4, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        # layer 1\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # layer 2\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# dense_layer를 1층 더 추가 + dropout 0.5\n",
        "class SimpleClassifier_Layer_dr05(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base//2)\n",
        "        self.dense2 = nn.Linear(classifier_hidden_size_base//2, classifier_hidden_size_base//4)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_up)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base//4, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        # layer 1\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # layer 2\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d_FiU8n07u-"
      },
      "source": [
        "## ELECTRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx2lFd3g07u-"
      },
      "source": [
        "#### baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3jlcACn07u-"
      },
      "outputs": [],
      "source": [
        "# category baseline\n",
        "class ElectraBaseClassifier_Cate_Base(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_Base, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity baseline\n",
        "class ElectraBaseClassifier_Pola_Base(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_Base, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOhFq7VR07u_"
      },
      "source": [
        "#### only layer 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9SVioy707u_"
      },
      "outputs": [],
      "source": [
        "# category layer 추가\n",
        "class ElectraBaseClassifier_Cate_Layer(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_Layer, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity layer 추가\n",
        "class ElectraBaseClassifier_Pola_Layer(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_Layer, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3kegOE807u_"
      },
      "source": [
        "#### only dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5aQF1-s07u_"
      },
      "outputs": [],
      "source": [
        "# category dropout 0.5\n",
        "class ElectraBaseClassifier_Cate_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity dropout 0.5\n",
        "class ElectraBaseClassifier_Pola_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTE5Y5mF07u_"
      },
      "source": [
        "#### layer + dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxbz2vmd07vA"
      },
      "outputs": [],
      "source": [
        "# category layer 추가 + dropout 0.5\n",
        "class ElectraBaseClassifier_Cate_Layer_Dropout05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_Layer_Dropout05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity layer 추가 + dropout 0.5\n",
        "class ElectraBaseClassifier_Pola_Layer_Dropout05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_Layer_Dropout05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9RDoew07vA"
      },
      "source": [
        "#### hidden_size_up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5Gd1bv_07vA"
      },
      "outputs": [],
      "source": [
        "# category hidden_size_up\n",
        "class ElectraBaseClassifier_Cate_hiddenup(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_hiddenup, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_up(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity hidden_size_up\n",
        "class ElectraBaseClassifier_Pola_hiddenup(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_hiddenup, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_up(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY5dmyQy07vA"
      },
      "source": [
        "#### only hidden_size_down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WKqzLl007vA"
      },
      "outputs": [],
      "source": [
        "# category hidden down\n",
        "class ElectraBaseClassifier_Cate_hiddendown(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_hiddendown, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity hidden down\n",
        "class ElectraBaseClassifier_Pola_hiddendown(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_hiddendown, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVkzaWl07vA"
      },
      "source": [
        "#### hidden_size_down + dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp_ZDy0S07vA"
      },
      "outputs": [],
      "source": [
        "# category hidden down + dropout 0.5\n",
        "class ElectraBaseClassifier_Cate_hiddendown_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_hiddendown_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity hidden down + dropout 0.5\n",
        "class ElectraBaseClassifier_Pola_hiddendown_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_hiddendown_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYQmoePU07vB"
      },
      "source": [
        "## RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSVeFQfY07vB"
      },
      "outputs": [],
      "source": [
        "# category baseline(roberta)\n",
        "class RobertaBaseClassifier(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(RobertaBaseClassifier, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.roberta = AutoModel.from_pretrained(base_model_roberta) \n",
        "        self.roberta.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdAFQ8IP07vB"
      },
      "source": [
        "## DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-of-YiZ07vB"
      },
      "outputs": [],
      "source": [
        "# category baseline(deberta)\n",
        "class DebertaBaseClassifier(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(DebertaBaseClassifier, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.deberta = AutoModel.from_pretrained(base_model_deberta)\n",
        "        self.deberta.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.deberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9PwPQ-3x3jw"
      },
      "source": [
        "# 모델 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB7f2eFd07vB"
      },
      "source": [
        "## category(entitiy) 학습\n",
        "\n",
        "* 자신이 원하는 파라미터를 수정한 목차로 넘어가서 함수를 실행한다.\n",
        "* **모든 함수 명이 같으니 실수 하지 않도록 주의하도록 하자.**\n",
        "* 함수를 실행한 후, \"cate 학습 시작\" 목차로 넘어가서 train을 시작하면 된다! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_JdBSOd07vB"
      },
      "source": [
        "### ELECTRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUMX29p407vC"
      },
      "source": [
        "#### base category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3gcvB8D07vC"
      },
      "outputs": [],
      "source": [
        "# base electra\n",
        "def train_entity_analysis():\n",
        "    \n",
        "    # data의 부족으로 valid data를 만들지 않아 따로 learning_rate을 직접 핸들링함. \n",
        "    # global learning_rate \n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_Base(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_Base\")\n",
        "        \n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "\n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "\n",
        "            # learning_rate를 직접 핸들링 할 때 주석 해제 후 사용\n",
        "            # learning_rate = learning_rate * 0.1\n",
        "            \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_base_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUVWEP2f07vC"
      },
      "source": [
        "#### only layer 추가 category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N7MGPJ407vC"
      },
      "outputs": [],
      "source": [
        "# only_layer electra\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_Layer(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_Layer\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_layer_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a385nnSy07vC"
      },
      "source": [
        "#### only dropout 0.5 category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC5qubdW07vC"
      },
      "outputs": [],
      "source": [
        "# only_dropout 0.5 electra\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_dr05(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_dr05\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_dr05_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC55ejU307vD"
      },
      "source": [
        "#### layer + dropout 0.5 category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaFUPZmE07vD"
      },
      "outputs": [],
      "source": [
        "# layer + dropout 0.5 electra\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_Layer_Dropout05(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_Layer_Dropout05\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_layer_dr05_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSqlxeF307vD"
      },
      "source": [
        "#### hidden_size_up category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbd7NH6v07vD"
      },
      "outputs": [],
      "source": [
        "# hidden_size_up electra\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_hiddenup(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_hiddenup\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_hiddenup_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgNiz2Z007vD"
      },
      "source": [
        "#### hidden_size down category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuzAWgsV07vD"
      },
      "outputs": [],
      "source": [
        "# hidden_size_down\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_hiddendown(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_hiddendown\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_hiddendown_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g6X_X1a07vE"
      },
      "source": [
        "#### hidden_size_down + dropout 0.5 category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1iFKSMT07vE"
      },
      "outputs": [],
      "source": [
        "# hidden_size_down + dropout 0.5\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = ElectraBaseClassifier_Cate_hiddendown_dr05(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Cate_hiddendown_dr05\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'elec_cate_hiddendown_dr05_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLynn11h07vE"
      },
      "source": [
        "### RoBERTa category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP_C2eZ607vE"
      },
      "outputs": [],
      "source": [
        "# base roberta\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_roberta)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_robe)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = RobertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"RobertaBaseClassifier\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'robe_cate_base_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0Lgbp0s07vE"
      },
      "source": [
        "### DeBERTa category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELv-WYnd07vE"
      },
      "outputs": [],
      "source": [
        "# base deberta\n",
        "def train_entity_analysis():\n",
        "\n",
        "    print('train_entity_analysis')\n",
        "    print('category_extraction model would be saved at ', category_extraction_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_cate_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_deberta)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    entity_property_train_data, _ = get_dataset(train_data, tokenizer, max_len_debe)\n",
        "    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n",
        "                                  batch_size=batch_size)\n",
        "\n",
        "    print('loading model')\n",
        "    entity_property_model = DebertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # entity_property_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    entity_property_model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # entity_property_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        entity_property_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n",
        "        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n",
        "\n",
        "    entity_property_optimizer = AdamW(\n",
        "        entity_property_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(entity_property_train_dataloader)\n",
        "\n",
        "    entity_property_scheduler = get_linear_schedule_with_warmup(\n",
        "        entity_property_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        entity_property_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # entity_property train\n",
        "        entity_property_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"DebertaBaseClassifier\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(entity_property_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(entity_property_train_dataloader))\n",
        "        \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            entity_property_model.zero_grad()\n",
        "\n",
        "            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            entity_property_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n",
        "            entity_property_optimizer.step()\n",
        "            entity_property_scheduler.step()\n",
        "\n",
        "        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = category_extraction_model_path + 'debe_cate_base_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(entity_property_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohA76h6X07vF"
      },
      "source": [
        "## category 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBw95cRt07vF"
      },
      "outputs": [],
      "source": [
        "train_entity_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsE7qYMU07vF"
      },
      "source": [
        "## polarity 학습\n",
        "\n",
        "* 자신이 원하는 파라미터를 수정한 목차로 넘어가서 함수를 실행한다.\n",
        "* **모든 함수 명이 같으니 실수 하지 않도록 주의하도록 하자.**\n",
        "* 함수를 실행한 후, \"pola 학습 시작\" 목차로 넘어가서 train을 시작하면 된다! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTawbKa807vF"
      },
      "source": [
        "### ELECTRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1UwBIyg07vF"
      },
      "source": [
        "#### base polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR-lhT7I07vF"
      },
      "outputs": [],
      "source": [
        "# base electra\n",
        "def train_polarity_analysis() :\n",
        "\n",
        "    # data의 부족으로 valid data를 만들지 않아 따로 learning_rate을 직접 핸들링함. \n",
        "    # global learning_rate \n",
        "\n",
        "    print('train_polarity_analysis')\n",
        "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_pola_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    _, polarity_train_data = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    print('loading model')\n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # polarity_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    polarity_model.to(device)\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # polarity_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        polarity_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
        "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
        "\n",
        "    polarity_optimizer = AdamW(\n",
        "        polarity_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(polarity_train_dataloader)\n",
        "\n",
        "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
        "        polarity_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        polarity_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # polarity train\n",
        "        polarity_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Pola_Base\")\n",
        "        \n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "\n",
        "        for step, batch in enumerate(polarity_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(polarity_train_dataloader))\n",
        "                \n",
        "            # learning_rate를 직접 핸들링 할 때 주석 해제 후 사용\n",
        "            # learning_rate = learning_rate * 0.1\n",
        "            \n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            polarity_model.zero_grad()\n",
        "\n",
        "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            polarity_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
        "            polarity_optimizer.step()\n",
        "            polarity_scheduler.step()\n",
        "\n",
        "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = polarity_classification_model_path + 'elec_pola_base_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uny0mDe_07vF"
      },
      "source": [
        "#### only layer 추가 polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn3k03tY07vG"
      },
      "outputs": [],
      "source": [
        "# layer electra\n",
        "def train_polarity_analysis() :\n",
        "\n",
        "    print('train_polarity_analysis')\n",
        "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_pola_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    _, polarity_train_data = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    print('loading model')\n",
        "    polarity_model = ElectraBaseClassifier_Pola_Layer(len(polarity_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # polarity_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    polarity_model.to(device)\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # polarity_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        polarity_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
        "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
        "\n",
        "    polarity_optimizer = AdamW(\n",
        "        polarity_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(polarity_train_dataloader)\n",
        "\n",
        "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
        "        polarity_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        polarity_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # polarity train\n",
        "        polarity_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Pola_Layer\")\n",
        "        \n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(polarity_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(polarity_train_dataloader))\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            polarity_model.zero_grad()\n",
        "\n",
        "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            polarity_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
        "            polarity_optimizer.step()\n",
        "            polarity_scheduler.step()\n",
        "\n",
        "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = polarity_classification_model_path + 'elec_pola_layer_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzNR6WOM07vG"
      },
      "source": [
        "#### dropout 0.5 polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_Eh3Tpa07vG"
      },
      "outputs": [],
      "source": [
        "# dropout 0.5 electra\n",
        "def train_polarity_analysis() :\n",
        "\n",
        "    print('train_polarity_analysis')\n",
        "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_pola_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    _, polarity_train_data = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    print('loading model')\n",
        "    polarity_model = ElectraBaseClassifier_Pola_dr05(len(polarity_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # polarity_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    polarity_model.to(device)\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # polarity_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        polarity_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
        "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
        "\n",
        "    polarity_optimizer = AdamW(\n",
        "        polarity_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(polarity_train_dataloader)\n",
        "\n",
        "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
        "        polarity_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        polarity_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # polarity train\n",
        "        polarity_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Pola_dr05\")\n",
        "        \n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "\n",
        "        for step, batch in enumerate(polarity_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(polarity_train_dataloader))\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            polarity_model.zero_grad()\n",
        "\n",
        "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            polarity_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
        "            polarity_optimizer.step()\n",
        "            polarity_scheduler.step()\n",
        "\n",
        "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = polarity_classification_model_path + 'elec_pola_dr05_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rwRn4EB07vG"
      },
      "source": [
        "#### layer + dropout 0.5 polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLZlAAvu07vG"
      },
      "outputs": [],
      "source": [
        "# layer + dropout 0.5 electra\n",
        "def train_polarity_analysis() :\n",
        "\n",
        "    print('train_polarity_analysis')\n",
        "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_pola_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    _, polarity_train_data = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    print('loading model')\n",
        "    polarity_model = ElectraBaseClassifier_Pola_Layer_Dropout05(len(polarity_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # polarity_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    polarity_model.to(device)\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # polarity_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        polarity_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
        "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
        "\n",
        "    polarity_optimizer = AdamW(\n",
        "        polarity_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(polarity_train_dataloader)\n",
        "\n",
        "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
        "        polarity_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        polarity_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # polarity train\n",
        "        polarity_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Pola_Layer_Dropout05\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(polarity_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(polarity_train_dataloader))\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            polarity_model.zero_grad()\n",
        "\n",
        "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            polarity_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
        "            polarity_optimizer.step()\n",
        "            polarity_scheduler.step()\n",
        "\n",
        "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = polarity_classification_model_path + 'elec_pola_layer_dr05_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4NZeQlP07vH"
      },
      "source": [
        "#### hidden_size_down polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1YrkSCc07vH"
      },
      "outputs": [],
      "source": [
        "# hidden_size_down electra\n",
        "def train_polarity_analysis() :\n",
        "\n",
        "    print('train_polarity_analysis')\n",
        "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_pola_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    _, polarity_train_data = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    print('loading model')\n",
        "    polarity_model = ElectraBaseClassifier_Pola_hiddendown(len(polarity_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # polarity_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    polarity_model.to(device)\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # polarity_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        polarity_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
        "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
        "\n",
        "    polarity_optimizer = AdamW(\n",
        "        polarity_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(polarity_train_dataloader)\n",
        "\n",
        "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
        "        polarity_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        polarity_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # polarity train\n",
        "        polarity_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Pola_hiddendown\")\n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        \n",
        "        for step, batch in enumerate(polarity_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(polarity_train_dataloader))\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            polarity_model.zero_grad()\n",
        "\n",
        "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            polarity_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
        "            polarity_optimizer.step()\n",
        "            polarity_scheduler.step()\n",
        "\n",
        "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = polarity_classification_model_path + 'elec_pola_hiddendown_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6wQpicv07vH"
      },
      "source": [
        "#### hidden_size_down + dropout 0.5 polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqLfvM4y07vH"
      },
      "outputs": [],
      "source": [
        "# hidden_size_down + dropout 0.5 electra\n",
        "def train_polarity_analysis() :\n",
        "\n",
        "    print('train_polarity_analysis')\n",
        "    print('polarity model would be saved at ', polarity_classification_model_path)\n",
        "\n",
        "    print('loading train data')\n",
        "    train_data = jsonlload(train_pola_data_path)\n",
        "\n",
        "    print('tokenizing train data')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    print('We have added', num_added_toks, 'tokens')\n",
        "    _, polarity_train_data = get_dataset(train_data, tokenizer, max_len_elec)\n",
        "    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    print('loading model')\n",
        "    polarity_model = ElectraBaseClassifier_Pola_hiddendown_dr05(len(polarity_id_to_name), len(tokenizer))\n",
        "\n",
        "    # ====================================================================================================== #\n",
        "    # 특정 epoch의 pt파일을 불러와서 이어서 학습할 때 사용 / torch.load(\"이 부분에 pt파일 경로 넣기\")\n",
        "    # polarity_model.load_state_dict(torch.load(\"/content/drive/MyDrive/sample.pt\"))\n",
        "    polarity_model.to(device)\n",
        "\n",
        "\n",
        "    print('end loading')\n",
        "\n",
        "    # polarity_model_optimizer_setting\n",
        "    FULL_FINETUNING = True\n",
        "    if FULL_FINETUNING:\n",
        "        polarity_param_optimizer = list(polarity_model.named_parameters())\n",
        "        no_decay = ['bias', 'gamma', 'beta']\n",
        "        polarity_optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.01},\n",
        "            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n",
        "             'weight_decay_rate': 0.0}\n",
        "        ]\n",
        "    else:\n",
        "        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n",
        "        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n",
        "\n",
        "    polarity_optimizer = AdamW(\n",
        "        polarity_optimizer_grouped_parameters,\n",
        "        lr=learning_rate,\n",
        "        eps=eps\n",
        "    )\n",
        "    epochs = num_train_epochs\n",
        "    max_grad_norm = 1.0\n",
        "    total_steps = epochs * len(polarity_train_dataloader)\n",
        "\n",
        "    polarity_scheduler = get_linear_schedule_with_warmup(\n",
        "        polarity_optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for _ in trange(epochs, desc=\"Epoch\"):\n",
        "        polarity_model.train()\n",
        "        epoch_step += 1\n",
        "        print(\"epoch_step ==>\", epoch_step)\n",
        "\n",
        "        # polarity train\n",
        "        polarity_total_loss = 0\n",
        "        \n",
        "        # train 데이터와 학습시키는 모델 이름 출력\n",
        "        print(train_cate_data_path)\n",
        "        print(base_model_elec)\n",
        "        print(\"ElectraBaseClassifier_Pola_hiddendown_dr05\")\n",
        "        \n",
        "\n",
        "        #====================================================================================================== #\n",
        "        # step : 모델 학습 시킬때 학습진행과정 확인하기 위한 과정  \n",
        "        for step, batch in enumerate(polarity_train_dataloader):\n",
        "            if step%1000==0:\n",
        "                print(step, \"/\", len(polarity_train_dataloader))\n",
        "\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            polarity_model.zero_grad()\n",
        "\n",
        "            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            polarity_total_loss += loss.item()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n",
        "            polarity_optimizer.step()\n",
        "            polarity_scheduler.step()\n",
        "\n",
        "        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n",
        "        print(\"Entity_Property_Epoch: \", epoch_step)\n",
        "        print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "\n",
        "        model_saved_path = polarity_classification_model_path + 'elec_pola_hiddendown_dr05_epoch_' + str(epoch_step) + '.pt'\n",
        "        torch.save(polarity_model.state_dict(), model_saved_path)\n",
        "\n",
        "    print(\"training is done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3D62sv607vI"
      },
      "source": [
        "## polarity 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UyiPK5r07vI"
      },
      "outputs": [],
      "source": [
        "train_polarity_analysis()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ohA76h6X07vF",
        "n3D62sv607vI"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tf27')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d8015098d1dae84219b5f36314149636e9abcaade3eb06915d5127b55dee25b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
