{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgOShdW807u0"
      },
      "source": [
        "# install 하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJZ94pW107u3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU18_4go07u4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGBVKtsL07u4"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rICaNAspcMYc"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FMpmgnrcGiw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import trange\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import AdamW\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "from transformers import AutoModel, ElectraTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIL8vzIx6xd"
      },
      "source": [
        "# 모델 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXBcX7-H07u6"
      },
      "source": [
        "## 기본설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urIlJHaHcO4A"
      },
      "outputs": [],
      "source": [
        "\n",
        "PADDING_TOKEN = 1\n",
        "S_OPEN_TOKEN = 0\n",
        "S_CLOSE_TOKEN = 2\n",
        "\n",
        "do_eval=True\n",
        "\n",
        "# 모델을 학습 할 때 저장 되는 파일 경로\n",
        "category_extraction_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/category_extraction/'\n",
        "polarity_classification_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/polarity_classification/'\n",
        "\n",
        "\n",
        "# 저장된 모델 Weight 파일 경로\n",
        "test_category_extraction_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/category_extraction/category_sample.pt'\n",
        "test_polarity_classification_model_path = '/content/drive/MyDrive/korean_baseline/saved_model/polarity_classification/polarity_sample.pt'\n",
        "\n",
        "# 데이터 파일 경로\n",
        "train_cate_data_path = '/content/drive/MyDrive/train_sample(category).jsonl'\n",
        "train_pola_data_path = '/content/drive/MyDrive/train_sample(polarity).jsonl'\n",
        "test_data_path = '/content/drive/MyDrive/test_sample.jsonl'\n",
        "\n",
        "max_len_elec = 256\n",
        "max_len_debe = 256\n",
        "max_len_robe = 514\n",
        "\n",
        "# colab pro 환경에서 RoBERTa를 돌리게 될 경우 batch_size 수정 요망 ( Out of Memory 이슈 )\n",
        "batch_size = 32\n",
        "\n",
        "#ELECTRA\n",
        "base_model_elec = 'kykim/electra-kor-base'\n",
        "#RoBERTa\n",
        "base_model_roberta = 'xlm-roberta-base'\n",
        "#DeBERTa\n",
        "base_model_deberta = \"lighthouse/mdeberta-v3-base-kor-further\"\n",
        "\n",
        "learning_rate = 3e-6\n",
        "eps = 1e-8\n",
        "num_train_epochs = 30\n",
        "\n",
        "classifier_hidden_size_base = 768\n",
        "classifier_hidden_size_down = 384   # hidden_size down\n",
        "classifier_hidden_size_up = 1000    # hidden_size up\n",
        "\n",
        "classifier_dropout_prob_base = 0.1  # dropout = 0.1\n",
        "classifier_dropout_prob_up = 0.5    # dropout = 0.5\n",
        "\n",
        "# 카테고리의 수 = 25개\n",
        "entity_property_pair = [\n",
        "     '패키지/구성품#다양성','본품#인지도','브랜드#디자인',\n",
        "     '패키지/구성품#편의성','제품 전체#디자인', '제품 전체#품질',\n",
        "     '패키지/구성품#품질','패키지/구성품#일반','본품#일반',\n",
        "     '패키지/구성품#디자인','본품#편의성','브랜드#품질',\n",
        "     '브랜드#인지도','본품#다양성','본품#디자인',\n",
        "     '제품 전체#다양성','본품#품질','제품 전체#인지도',\n",
        "     '패키지/구성품#가격','본품#가격','제품 전체#가격',\n",
        "     '브랜드#가격','브랜드#일반','제품 전체#일반','제품 전체#편의성'\n",
        "     ]\n",
        "\n",
        "tf_id_to_name = ['True', 'False']\n",
        "tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n",
        "\n",
        "polarity_id_to_name = ['positive', 'negative', 'neutral']\n",
        "polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZSDDZfZr7y4"
      },
      "outputs": [],
      "source": [
        "def jsonload(fname, encoding=\"utf-8\"):\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        j = json.load(f)\n",
        "\n",
        "    return j\n",
        "\n",
        "# json 개체를 파일이름으로 깔끔하게 저장\n",
        "def jsondump(j, fname):\n",
        "    with open(fname, \"w\", encoding=\"UTF8\") as f:\n",
        "        json.dump(j, f, ensure_ascii=False)\n",
        "\n",
        "# jsonl 파일 읽어서 list에 저장\n",
        "def jsonlload(fname, encoding=\"utf-8\"):\n",
        "    json_list = []\n",
        "    with open(fname, encoding=encoding) as f:\n",
        "        for line in f.readlines():\n",
        "            json_list.append(json.loads(line))\n",
        "    return json_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTQwoV4RtSRs"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n",
        "\n",
        "    entity_property_data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': []\n",
        "    }\n",
        "    polarity_data_dict = {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    for pair in entity_property_pair:\n",
        "        isPairInOpinion = False\n",
        "        if pd.isna(form):\n",
        "            break\n",
        "        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n",
        "        for annotation in annotations:\n",
        "            entity_property = annotation[0]\n",
        "            polarity = annotation[2]\n",
        "\n",
        "            if polarity == '------------':\n",
        "                continue\n",
        "\n",
        "            if entity_property == pair:\n",
        "                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n",
        "\n",
        "                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n",
        "\n",
        "                isPairInOpinion = True\n",
        "                break\n",
        "\n",
        "        if isPairInOpinion is False:\n",
        "            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n",
        "            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n",
        "            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n",
        "\n",
        "    return entity_property_data_dict, polarity_data_dict\n",
        "\n",
        "\n",
        "def get_dataset(raw_data, tokenizer, max_len):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    token_labels_list = []\n",
        "\n",
        "    polarity_input_ids_list = []\n",
        "    polarity_attention_mask_list = []\n",
        "    polarity_token_labels_list = []\n",
        "\n",
        "    for utterance in raw_data:\n",
        "        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n",
        "        input_ids_list.extend(entity_property_data_dict['input_ids'])\n",
        "        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n",
        "        token_labels_list.extend(entity_property_data_dict['label'])\n",
        "\n",
        "        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n",
        "        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n",
        "        polarity_token_labels_list.extend(polarity_data_dict['label'])\n",
        "\n",
        "    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n",
        "                         torch.tensor(token_labels_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n",
        "                         torch.tensor(polarity_token_labels_list))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ2NE8pIxdEY"
      },
      "outputs": [],
      "source": [
        "def evaluation(y_true, y_pred, label_len):\n",
        "    count_list = [0]*label_len\n",
        "    hit_list = [0]*label_len\n",
        "    for i in range(len(y_true)):\n",
        "        count_list[y_true[i]] += 1\n",
        "        if y_true[i] == y_pred[i]:\n",
        "            hit_list[y_true[i]] += 1\n",
        "    acc_list = []\n",
        "\n",
        "    for i in range(label_len):\n",
        "        acc_list.append(hit_list[i]/count_list[i])\n",
        "\n",
        "    print(count_list)\n",
        "    print(hit_list)\n",
        "    print(acc_list)\n",
        "    print('accuracy: ', (sum(hit_list) / sum(count_list)))\n",
        "    print('macro_accuracy: ', sum(acc_list) / 3)\n",
        "    # print(y_true)\n",
        "\n",
        "    y_true = list(map(int, y_true))\n",
        "    y_pred = list(map(int, y_pred))\n",
        "\n",
        "    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n",
        "    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n",
        "    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klOrDUrd28IM"
      },
      "outputs": [],
      "source": [
        "def evaluation_f1(true_data, pred_data):\n",
        "\n",
        "    true_data_list = true_data\n",
        "    pred_data_list = pred_data\n",
        "\n",
        "    ce_eval = {\n",
        "        'TP': 0,\n",
        "        'FP': 0,\n",
        "        'FN': 0,\n",
        "        'TN': 0\n",
        "    }\n",
        "\n",
        "    pipeline_eval = {\n",
        "        'TP': 0,\n",
        "        'FP': 0,\n",
        "        'FN': 0,\n",
        "        'TN': 0\n",
        "    }\n",
        "\n",
        "    for i in range(len(true_data_list)):\n",
        "\n",
        "        # TP, FN checking\n",
        "        is_ce_found = False\n",
        "        is_pipeline_found = False\n",
        "        for y_ano  in true_data_list[i]['annotation']:\n",
        "            y_category = y_ano[0]\n",
        "            y_polarity = y_ano[2]\n",
        "\n",
        "            for p_ano in pred_data_list[i]['annotation']:\n",
        "                p_category = p_ano[0]\n",
        "                p_polarity = p_ano[1]\n",
        "\n",
        "                if y_category == p_category:\n",
        "                    is_ce_found = True\n",
        "                    if y_polarity == p_polarity:\n",
        "                        is_pipeline_found = True\n",
        "\n",
        "                    break\n",
        "\n",
        "            if is_ce_found is True:\n",
        "                ce_eval['TP'] += 1\n",
        "            else:\n",
        "                ce_eval['FN'] += 1\n",
        "\n",
        "            if is_pipeline_found is True:\n",
        "                pipeline_eval['TP'] += 1\n",
        "            else:\n",
        "                pipeline_eval['FN'] += 1\n",
        "\n",
        "            is_ce_found = False\n",
        "            is_pipeline_found = False\n",
        "\n",
        "        # FP checking\n",
        "        for p_ano in pred_data_list[i]['annotation']:\n",
        "            p_category = p_ano[0]\n",
        "            p_polarity = p_ano[1]\n",
        "\n",
        "            for y_ano  in true_data_list[i]['annotation']:\n",
        "                y_category = y_ano[0]\n",
        "                y_polarity = y_ano[2]\n",
        "\n",
        "                if y_category == p_category:\n",
        "                    is_ce_found = True\n",
        "                    if y_polarity == p_polarity:\n",
        "                        is_pipeline_found = True\n",
        "\n",
        "                    break\n",
        "\n",
        "            if is_ce_found is False:\n",
        "                ce_eval['FP'] += 1\n",
        "\n",
        "            if is_pipeline_found is False:\n",
        "                pipeline_eval['FP'] += 1\n",
        "            is_ce_found = False\n",
        "            is_pipeline_found = False\n",
        "\n",
        "    ce_precision = ce_eval['TP']/(ce_eval['TP']+ce_eval['FP'])\n",
        "    ce_recall = ce_eval['TP']/(ce_eval['TP']+ce_eval['FN'])\n",
        "\n",
        "    ce_result = {\n",
        "        'Precision': ce_precision,\n",
        "        'Recall': ce_recall,\n",
        "        'F1': 2*ce_recall*ce_precision/(ce_recall+ce_precision)\n",
        "    }\n",
        "\n",
        "    pipeline_precision = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FP'])\n",
        "    pipeline_recall = pipeline_eval['TP']/(pipeline_eval['TP']+pipeline_eval['FN'])\n",
        "\n",
        "    pipeline_result = {\n",
        "        'Precision': pipeline_precision,\n",
        "        'Recall': pipeline_recall,\n",
        "        'F1': 2*pipeline_recall*pipeline_precision/(pipeline_recall+pipeline_precision)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'category extraction result': ce_result,\n",
        "        'entire pipeline result': pipeline_result\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-xisSs07u9"
      },
      "source": [
        "## SimpleClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6cjejoy07u9"
      },
      "outputs": [],
      "source": [
        "# baseline\n",
        "class SimpleClassifier_Base(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# hidden_size를 1000으로 up\n",
        "class SimpleClassifier_Hidden_up(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_up)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_up, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x        \n",
        "\n",
        "# hidden_size를 384로 down\n",
        "class SimpleClassifier_Hidden_down(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_down)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_down, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x        \n",
        "\n",
        "# dropout 0.5\n",
        "class SimpleClassifier_dr05(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_up)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "# hidden_size를 384로 down + dropout 0.5\n",
        "class SimpleClassifier_Hidden_down_dr05(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_down)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_up)\n",
        "        self.output = nn.Linear(classifier_hidden_size_down, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x        \n",
        "\n",
        "# dense_layer를 1층 더 추가 (dropout0.1)\n",
        "class SimpleClassifier_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base//2)\n",
        "        self.dense2 = nn.Linear(classifier_hidden_size_base//2, classifier_hidden_size_base//4)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_base)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base//4, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        # layer 1\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # layer 2\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# dense_layer를 1층 더 추가 + dropout 0.5\n",
        "class SimpleClassifier_Layer_dr05(nn.Module):\n",
        "\n",
        "    def __init__(self, num_label):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.Linear(classifier_hidden_size_base, classifier_hidden_size_base//2)\n",
        "        self.dense2 = nn.Linear(classifier_hidden_size_base//2, classifier_hidden_size_base//4)\n",
        "        self.dropout = nn.Dropout(classifier_dropout_prob_up)\n",
        "        self.output = nn.Linear(classifier_hidden_size_base//4, num_label)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features[:, 0, :]\n",
        "        # layer 1\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # layer 2\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d_FiU8n07u-"
      },
      "source": [
        "## ELECTRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx2lFd3g07u-"
      },
      "source": [
        "#### baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3jlcACn07u-"
      },
      "outputs": [],
      "source": [
        "# category baseline\n",
        "class ElectraBaseClassifier_Cate_Base(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_Base, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity baseline\n",
        "class ElectraBaseClassifier_Pola_Base(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_Base, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOhFq7VR07u_"
      },
      "source": [
        "#### only layer 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9SVioy707u_"
      },
      "outputs": [],
      "source": [
        "# category layer 추가\n",
        "class ElectraBaseClassifier_Cate_Layer(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_Layer, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity layer 추가\n",
        "class ElectraBaseClassifier_Pola_Layer(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_Layer, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3kegOE807u_"
      },
      "source": [
        "#### only dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5aQF1-s07u_"
      },
      "outputs": [],
      "source": [
        "# category dropout 0.5\n",
        "class ElectraBaseClassifier_Cate_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity dropout 0.5\n",
        "class ElectraBaseClassifier_Pola_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTE5Y5mF07u_"
      },
      "source": [
        "#### layer + dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxbz2vmd07vA"
      },
      "outputs": [],
      "source": [
        "# category layer 추가 + dropout 0.5\n",
        "class ElectraBaseClassifier_Cate_Layer_Dropout05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_Layer_Dropout05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity layer 추가 + dropout 0.5\n",
        "class ElectraBaseClassifier_Pola_Layer_Dropout05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_Layer_Dropout05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Layer_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9RDoew07vA"
      },
      "source": [
        "#### hidden_size_up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5Gd1bv_07vA"
      },
      "outputs": [],
      "source": [
        "# category hidden_size_up\n",
        "class ElectraBaseClassifier_Cate_hiddenup(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_hiddenup, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_up(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity hidden_size_up\n",
        "class ElectraBaseClassifier_Pola_hiddenup(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_hiddenup, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_up(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY5dmyQy07vA"
      },
      "source": [
        "#### only hidden_size_down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WKqzLl007vA"
      },
      "outputs": [],
      "source": [
        "# category hidden down\n",
        "class ElectraBaseClassifier_Cate_hiddendown(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_hiddendown, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity hidden down\n",
        "class ElectraBaseClassifier_Pola_hiddendown(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_hiddendown, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVkzaWl07vA"
      },
      "source": [
        "#### hidden_size_down + dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp_ZDy0S07vA"
      },
      "outputs": [],
      "source": [
        "# category hidden down + dropout 0.5\n",
        "class ElectraBaseClassifier_Cate_hiddendown_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Cate_hiddendown_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n",
        "\n",
        "# polarity hidden down + dropout 0.5\n",
        "class ElectraBaseClassifier_Pola_hiddendown_dr05(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(ElectraBaseClassifier_Pola_hiddendown_dr05, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.electra = AutoModel.from_pretrained(base_model_elec)\n",
        "        self.electra.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Hidden_down_dr05(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.electra(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYQmoePU07vB"
      },
      "source": [
        "## RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSVeFQfY07vB"
      },
      "outputs": [],
      "source": [
        "# category baseline(roberta)\n",
        "class RobertaBaseClassifier(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(RobertaBaseClassifier, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.roberta = AutoModel.from_pretrained(base_model_roberta) \n",
        "        self.roberta.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.roberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdAFQ8IP07vB"
      },
      "source": [
        "## DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-of-YiZ07vB"
      },
      "outputs": [],
      "source": [
        "# category baseline(deberta)\n",
        "class DebertaBaseClassifier(nn.Module):\n",
        "    def __init__(self, num_label, len_tokenizer):\n",
        "        super(DebertaBaseClassifier, self).__init__()\n",
        "\n",
        "        self.num_label = num_label\n",
        "        self.deberta = AutoModel.from_pretrained(base_model_deberta)\n",
        "        self.deberta.resize_token_embeddings(len_tokenizer)\n",
        "\n",
        "        self.labels_classifier = SimpleClassifier_Base(self.num_label)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.deberta(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=None\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.labels_classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_label),\n",
        "                                                labels.view(-1))\n",
        "\n",
        "        return loss, logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfpZsR4o28S2"
      },
      "source": [
        "# 모델평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOrHEU8u1eLv"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnsM80Uh07vI"
      },
      "source": [
        "### base(ELECTRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMtw1JVt28Kv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_from_korean_form_kelec(tokenizer_kelec, ce_model, pc_model, data):\n",
        "\n",
        "    ce_model.to(device)\n",
        "    ce_model.eval()\n",
        "    for idx, sentence in enumerate(data):\n",
        "        if idx % 10 == 0:\n",
        "            print(idx, \"/\", len(data))\n",
        "\n",
        "        form = sentence['sentence_form']\n",
        "        sentence['annotation'] = []\n",
        "        if type(form) != str:\n",
        "            print(\"form type is arong: \", form)\n",
        "            continue\n",
        "        for pair in entity_property_pair:\n",
        "            \n",
        "            tokenized_data_kelec = tokenizer_kelec(form, pair, padding='max_length', max_length=256, truncation=True)\n",
        "            input_ids_kelec = torch.tensor([tokenized_data_kelec['input_ids']]).to(device)\n",
        "            attention_mask_kelec = torch.tensor([tokenized_data_kelec['attention_mask']]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids_kelec, attention_mask_kelec)\n",
        "\n",
        "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
        "\n",
        "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
        "\n",
        "            if ce_result == 'True':\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids_kelec, attention_mask_kelec)\n",
        "\n",
        "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "                sentence['annotation'].append([pair, pc_result])\n",
        "\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ca3C_PX07vI"
      },
      "source": [
        "### ELECTRA + Force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2kfj3Bs6v1A"
      },
      "source": [
        "Force : 빈칸( '[ ]' 에 대해서 가장 높은 확률의 카테고리를 강제로 뽑아내는 방법 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA1LEIAG07vI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_from_korean_form_kelec_forcing(tokenizer_kelec, ce_model, pc_model, data):\n",
        "\n",
        "    ce_model.to(device)\n",
        "    ce_model.eval()\n",
        "    for idx, sentence in enumerate(data):\n",
        "        if idx % 10 == 0:\n",
        "            print(idx, \"/\", len(data))\n",
        "\n",
        "        form = sentence['sentence_form']\n",
        "        sentence['annotation'] = []\n",
        "        if type(form) != str:\n",
        "            print(\"form type is arong: \", form)\n",
        "            continue\n",
        "\n",
        "\n",
        "        \n",
        "        tmp = []\n",
        "        flag = False\n",
        "\n",
        "        for pair in entity_property_pair:\n",
        "            \n",
        "            tokenized_data = tokenizer_kelec(form, pair, padding='max_length', max_length=256, truncation=True)\n",
        "\n",
        "            input_ids = torch.tensor([tokenized_data['input_ids']]).to(device)\n",
        "            attention_mask = torch.tensor([tokenized_data['attention_mask']]).to(device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids, attention_mask)\n",
        "                \n",
        "                tmp.append( ce_logits[0][0] )\n",
        "\n",
        "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
        "            \n",
        "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
        "\n",
        "\n",
        "            if ce_result == 'True':\n",
        "                flag = True\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids, attention_mask)\n",
        "\n",
        "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "                sentence['annotation'].append([pair, pc_result])\n",
        "        \n",
        "        if flag == False:\n",
        "\n",
        "            tmp = torch.tensor(tmp)\n",
        "\n",
        "            pair = entity_property_pair[torch.argmax(tmp)]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, pc_logits = pc_model(input_ids, attention_mask)\n",
        "\n",
        "            pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "            pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "            sentence['annotation'].append([pair, pc_result])\n",
        "                \n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDkmK2v007vJ"
      },
      "source": [
        "### category : RoBERTa / polarity : ELECTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_Tx_qkk07vJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def predict_from_korean_form_roberta(tokenizer_roberta, tokenizer_kelec, ce_model, pc_model, data):\n",
        "\n",
        "    ce_model.to(device)\n",
        "    ce_model.eval()\n",
        "    for idx, sentence in enumerate(data):\n",
        "        if idx % 10 == 0:\n",
        "            print(idx, \"/\", len(data))\n",
        "\n",
        "        form = sentence['sentence_form']\n",
        "        sentence['annotation'] = []\n",
        "        if type(form) != str:\n",
        "            print(\"form type is arong: \", form)\n",
        "            continue\n",
        "        for pair in entity_property_pair:\n",
        "            \n",
        "\n",
        "            tokenized_data_roberta = tokenizer_roberta(form, pair, padding='max_length', max_length=514, truncation=True)\n",
        "            tokenized_data_kelec = tokenizer_kelec(form, pair, padding='max_length', max_length=256, truncation=True)\n",
        "\n",
        "            input_ids_roberta = torch.tensor([tokenized_data_roberta['input_ids']]).to(device)\n",
        "            attention_mask_roberta = torch.tensor([tokenized_data_roberta['attention_mask']]).to(device)\n",
        "\n",
        "            input_ids_kelec = torch.tensor([tokenized_data_kelec['input_ids']]).to(device)\n",
        "            attention_mask_kelec = torch.tensor([tokenized_data_kelec['attention_mask']]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids_roberta, attention_mask_roberta)\n",
        "\n",
        "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
        "\n",
        "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
        "\n",
        "            if ce_result == 'True':\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids_kelec, attention_mask_kelec)\n",
        "\n",
        "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "                sentence['annotation'].append([pair, pc_result])\n",
        "\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3hxHlf507vJ"
      },
      "source": [
        "### category : RoBERTa / polarity : ELECTRA + Force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3te7ws5K7KXu"
      },
      "source": [
        "Force : 빈칸( '[ ]' 에 대해서 가장 높은 확률의 카테고리를 강제로 뽑아내는 방법 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G82EqJYY07vJ"
      },
      "outputs": [],
      "source": [
        "def predict_from_korean_form_roberta_forcing(tokenizer_roberta, tokenizer_kelec, ce_model, pc_model, data):\n",
        "\n",
        "    ce_model.to(device)\n",
        "    ce_model.eval()\n",
        "    for idx, sentence in enumerate(data):\n",
        "        if idx % 10 == 0:\n",
        "            print(idx, \"/\", len(data))\n",
        "\n",
        "        form = sentence['sentence_form']\n",
        "        sentence['annotation'] = []\n",
        "        if type(form) != str:\n",
        "            print(\"form type is arong: \", form)\n",
        "            continue\n",
        "\n",
        "\n",
        "        tmp = []\n",
        "        flag = False\n",
        "\n",
        "        for pair in entity_property_pair:\n",
        "            \n",
        "            tokenized_data_kelec = tokenizer_kelec(form, pair, padding='max_length', max_length=256, truncation=True)\n",
        "            tokenized_data_roberta = tokenizer_roberta(form, pair, padding='max_length', max_length=514, truncation=True)\n",
        "\n",
        "            input_ids_kelec = torch.tensor([tokenized_data_kelec['input_ids']]).to(device)\n",
        "            attention_mask_kelec = torch.tensor([tokenized_data_kelec['attention_mask']]).to(device)\n",
        "\n",
        "            input_ids_roberta = torch.tensor([tokenized_data_roberta['input_ids']]).to(device)\n",
        "            attention_mask_roberta = torch.tensor([tokenized_data_roberta['attention_mask']]).to(device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids_roberta, attention_mask_roberta)\n",
        "                \n",
        "                tmp.append( ce_logits[0][0] )\n",
        "\n",
        "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
        "            \n",
        "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
        "\n",
        "\n",
        "            if ce_result == 'True':\n",
        "                flag = True\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids_kelec, attention_mask_kelec)\n",
        "\n",
        "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "                sentence['annotation'].append([pair, pc_result])\n",
        "        \n",
        "        if flag == False:\n",
        "\n",
        "            tmp = torch.tensor(tmp)\n",
        "\n",
        "            pair = entity_property_pair[torch.argmax(tmp)]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, pc_logits = pc_model(input_ids_kelec, attention_mask_kelec)\n",
        "\n",
        "            pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "            pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "            sentence['annotation'].append([pair, pc_result])\n",
        "                \n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvvloDdd07vJ"
      },
      "source": [
        "### category : DeBERTa / polarity : ELECTRA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfXAAxkM07vJ"
      },
      "outputs": [],
      "source": [
        "def predict_from_korean_form_deberta(tokenizer_deberta, tokenizer_kelec, ce_model, pc_model, data):\n",
        "\n",
        "    ce_model.to(device)\n",
        "    ce_model.eval()\n",
        "    for idx, sentence in enumerate(data):\n",
        "        if idx % 10 == 0:\n",
        "            print(idx, \"/\", len(data))\n",
        "\n",
        "        form = sentence['sentence_form']\n",
        "        sentence['annotation'] = []\n",
        "        if type(form) != str:\n",
        "            print(\"form type is arong: \", form)\n",
        "            continue\n",
        "        for pair in entity_property_pair:\n",
        "            \n",
        "\n",
        "            tokenized_data_deberta = tokenizer_deberta(form, pair, padding='max_length', max_length=256, truncation=True)\n",
        "            tokenized_data_kelec = tokenizer_kelec(form, pair, padding='max_length', max_length=256, truncation=True)\n",
        "\n",
        "            input_ids_deberta = torch.tensor([tokenized_data_deberta['input_ids']]).to(device)\n",
        "            attention_mask_deberta = torch.tensor([tokenized_data_deberta['attention_mask']]).to(device)\n",
        "\n",
        "            input_ids_kelec = torch.tensor([tokenized_data_kelec['input_ids']]).to(device)\n",
        "            attention_mask_kelec = torch.tensor([tokenized_data_kelec['attention_mask']]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, ce_logits = ce_model(input_ids_deberta, attention_mask_deberta)\n",
        "\n",
        "            ce_predictions = torch.argmax(ce_logits, dim = -1)\n",
        "\n",
        "            ce_result = tf_id_to_name[ce_predictions[0]]\n",
        "\n",
        "            if ce_result == 'True':\n",
        "                with torch.no_grad():\n",
        "                    _, pc_logits = pc_model(input_ids_kelec, attention_mask_kelec)\n",
        "\n",
        "                pc_predictions = torch.argmax(pc_logits, dim=-1)\n",
        "                pc_result = polarity_id_to_name[pc_predictions[0]]\n",
        "\n",
        "                sentence['annotation'].append([pair, pc_result])\n",
        "\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCeZJpIv07vJ"
      },
      "source": [
        "## Inference\n",
        "\n",
        "* 단일 모델\n",
        "* 여러 모델\n",
        " - 두 개의 모델\n",
        " - 복수 개의 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swXVAhN307vK"
      },
      "source": [
        "### 한개 모델 inference 진행 ( ELECTRA )\n",
        "단일 모델 path의 경로는 '모델 구축-기본 설정'에서 확인하기! \n",
        "1. test_category_extraction_model_path\n",
        "2. test_polarity_classification_model_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlVdtfGs07vK"
      },
      "outputs": [],
      "source": [
        "def test_sentiment_analysis():\n",
        "\n",
        "    tokenizer_kelec = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "\n",
        "    num_added_toks_kelec = tokenizer_kelec.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    test_data = jsonlload(test_data_path)\n",
        "\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_kelec, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = ElectraBaseClassifier_Cate_Base(len(tf_id_to_name), len(tokenizer_kelec))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    # print('F1 result: ', evaluation_f1(test_data, pred_data))\n",
        "    pred_data = predict_from_korean_form_kelec(tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data))\n",
        "\n",
        "    df = {'id' : [], 'sentence_form' : [], 'annotation' : []}\n",
        "\n",
        "    for i in range(len(pred_data)) :\n",
        "        df['id'].append(pred_data[i]['id'])\n",
        "        df['sentence_form'].append(pred_data[i]['sentence_form'])\n",
        "        df['annotation'].append(pred_data[i]['annotation'])\n",
        "    df_pred = pd.DataFrame(df)\n",
        "\n",
        "\n",
        "    with open('Inference_sample.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred) ):\n",
        "            tmp = str(df_pred['annotation'][i]).replace(\"\\'\", \"\\\"\").replace('None', 'null')\n",
        "            file.write(  '{'+'\\\"id\\\": \\\"nikluge-sa-2022-test-{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                .format( str(i+1).zfill(5)  ,   df_pred['sentence_form'][i], tmp ) +'}' ) \n",
        "            file.write(\"\\n\")\n",
        "\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkHmi-zc07vK"
      },
      "outputs": [],
      "source": [
        "test_sentiment_analysis() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUGFyC7807vK"
      },
      "source": [
        "### 두개 모델을 불러 inference 진행\n",
        "\n",
        "* Category를 DeBERTa 가 아닌 RoBERTa로 찍고 싶다면 **\"주석 처리된 roberta 파트를 주석 해제\"** 하고 deberta 파트는 주석 처리를 하면 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnuZDZpsqT1M"
      },
      "outputs": [],
      "source": [
        "#Category\n",
        "\n",
        "#DeBERTa\n",
        "test_category_extraction_model_path_deberta = '/content/drive/MyDrive/deberta_sample.pt'\n",
        "#RoBERTa\n",
        "test_category_extraction_model_path_roberta = '/content/drive/MyDrive/roberta_sample.pt'\n",
        "\n",
        "#Polarity ( ELECTRA )\n",
        "test_polarity_classification_model_path = '/content/drive/MyDrive/sample.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWGKdE1X07vK"
      },
      "outputs": [],
      "source": [
        "def test_sentiment_analysis():\n",
        "\n",
        "\n",
        "    tokenizer_deberta = AutoTokenizer.from_pretrained(base_model_deberta)\n",
        "    # tokenizer_roberta = AutoTokenizer.from_pretrained(base_model_roberta)\n",
        "    tokenizer_kelec = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "\n",
        "    num_added_toks_deberta = tokenizer_deberta.add_special_tokens(special_tokens_dict)\n",
        "    # num_added_toks_roberta = tokenizer_roberta.add_special_tokens(special_tokens_dict)\n",
        "    num_added_toks_kelec = tokenizer_kelec.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    test_data = jsonlload(test_data_path)\n",
        "\n",
        "    entity_property_test_data_deberta, polarity_test_data_deberta = get_dataset(test_data, tokenizer_deberta, max_len_debe)\n",
        "    # entity_property_test_data_roberta, polarity_test_data_roberta = get_dataset(test_data, tokenizer_roberta, max_len_robe)\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_deberta, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    # entity_property_test_dataloader = DataLoader(entity_property_test_data_roberta, shuffle=True,\n",
        "    #                             batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = DebertaBaseClassifier(len(tf_id_to_name), len(tokenizer_deberta))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_deberta, map_location=device))\n",
        "    # model = RobertaBaseClassifier(len(tf_id_to_name), len(tokenizer_roberta))\n",
        "    # model.load_state_dict(torch.load(test_category_extraction_model_path_roberta, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "\n",
        "    # print('F1 result: ', evaluation_f1(test_data, pred_data))\n",
        "    pred_data = predict_from_korean_form_deberta(tokenizer_deberta ,tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data))\n",
        "    # pred_data = predict_from_korean_form_roberta(tokenizer_roberta ,tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data))\n",
        "\n",
        "    df = {'id' : [], 'sentence_form' : [], 'annotation' : []}\n",
        "\n",
        "    for i in range(len(pred_data)) :\n",
        "        df['id'].append(pred_data[i]['id'])\n",
        "        df['sentence_form'].append(pred_data[i]['sentence_form'])\n",
        "        df['annotation'].append(pred_data[i]['annotation'])\n",
        "    df_pred = pd.DataFrame(df)\n",
        "\n",
        "\n",
        "    with open('Inference_sample.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred) ):\n",
        "            tmp = str(df_pred['annotation'][i]).replace(\"\\'\", \"\\\"\").replace('None', 'null')\n",
        "            file.write(  '{'+'\\\"id\\\": \\\"nikluge-sa-2022-test-{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                .format( str(i+1).zfill(5)  ,   df_pred['sentence_form'][i], tmp ) +'}' ) \n",
        "            file.write(\"\\n\")\n",
        "\n",
        "    return df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1RxBw6z07vK"
      },
      "outputs": [],
      "source": [
        "test_sentiment_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms5MZYpR07vK"
      },
      "source": [
        "### 여러 모델을 불러 inference 진행\n",
        "\n",
        "* 각 모델 마다 잡아내는 문장의 annotation이 달라서 여러 모델을 불러서 inference를 진행하는 코드이다.\n",
        "* 현재 아래 코드는\n",
        "DeBERTa > ELECTRA(hidden_size_up) > ELECTRA > ELECTRA(dropout=0.5) > ELECTRA(Force)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNbrxBxC07vJ"
      },
      "source": [
        "Path ( test_category_path / test_polarity_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q57375IE07vJ"
      },
      "outputs": [],
      "source": [
        "#Category\n",
        "test_category_extraction_model_path_k_up = '/content/drive/MyDrive/sample(hiddenup).pt'\n",
        "test_category_extraction_model_path_k = '/content/drive/MyDrive/sample(category).pt'\n",
        "test_category_extraction_model_path_gpu_lr = '/content/drive/MyDrive/sample(learningrate).pt'\n",
        "test_category_extraction_model_path_deberta = '/content/drive/MyDrive/deberta_sample.pt'\n",
        "test_category_extraction_model_path_roberta = '/content/drive/MyDrive/roberta_sample.pt'\n",
        "test_category_extraction_model_path_origin = '/content/drive/MyDrive/sample(category).pt'\n",
        "\n",
        "#Polarity\n",
        "test_polarity_classification_model_path = '/content/drive/MyDrive/sample.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_YBI8o607vK"
      },
      "outputs": [],
      "source": [
        "test_data_path = '/content/drive/MyDrive/test_sample.jsonl'\n",
        "test_data_path_blank_first = '/content/Blank1.jsonl'\n",
        "test_data_path_blank_second = '/content/Blank2.jsonl'\n",
        "test_data_path_blank_third = '/content/Blank3.jsonl'\n",
        "test_data_path_blank_fourth = '/content/Blank4.jsonl'\n",
        "test_data_path_blank_fifth = '/content/Blank5.jsonl'\n",
        "test_data_path_blank_sixth = '/content/Blank6.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCJGgVoH07vK"
      },
      "outputs": [],
      "source": [
        "def Win():\n",
        "\n",
        "    print(\"Deberta!!\")\n",
        "\n",
        "    tokenizer_kelec = AutoTokenizer.from_pretrained(base_model_elec)\n",
        "    tokenizer_deberta = AutoTokenizer.from_pretrained(base_model_deberta)\n",
        "    tokenizer_roberta = AutoTokenizer.from_pretrained(base_model_roberta)\n",
        "\n",
        "    num_added_toks_kelec = tokenizer_kelec.add_special_tokens(special_tokens_dict)\n",
        "    num_added_toks_deberta = tokenizer_deberta.add_special_tokens(special_tokens_dict)\n",
        "    num_added_toks_roberta = tokenizer_roberta.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    test_data = jsonlload(test_data_path)\n",
        "\n",
        "    entity_property_test_data_deberta, polarity_test_data_deberta = get_dataset(test_data, tokenizer_deberta, max_len_debe)\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_deberta, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = DebertaBaseClassifier(len(tf_id_to_name), len(tokenizer_deberta))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_deberta, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form_deberta(tokenizer_deberta ,tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data))\n",
        "    df_pred_first = pd.DataFrame(pred_data)\n",
        "\n",
        "    with open('Blank1.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred_first) ):\n",
        "            if len(df_pred_first['annotation'][i]) == 0 :\n",
        "                tmp = '[[\"제품 전체#일반\", [null, 0, 0], \"positive\"]]'\n",
        "                file.write(  '{'+'\\\"id\\\": \\\"{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                    .format( df_pred_first['id'][i]  ,   df_pred_first['sentence_form'][i], tmp ) +'}' ) \n",
        "                file.write(\"\\n\")\n",
        "\n",
        "    print(\"K up!!\")\n",
        "    \n",
        "    test_data_blank_first = jsonlload(test_data_path_blank_first)\n",
        "\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data_blank_first, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_kelec, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = ElectraBaseClassifier_Cate_hiddenup(len(tf_id_to_name), len(tokenizer_kelec))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_k_up, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form_kelec(tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data_blank_first))\n",
        "    df_pred_second = pd.DataFrame(pred_data)\n",
        "\n",
        "    with open('Blank2.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred_second) ):\n",
        "            if len(df_pred_second['annotation'][i]) == 0 :\n",
        "                tmp = '[[\"제품 전체#일반\", [null, 0, 0], \"positive\"]]'\n",
        "                file.write(  '{'+'\\\"id\\\": \\\"{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                    .format( df_pred_second['id'][i]  ,   df_pred_second['sentence_form'][i], tmp ) +'}' ) \n",
        "                file.write(\"\\n\")\n",
        "\n",
        "    print(\"Original!!\")\n",
        "\n",
        "    test_data_blank_second = jsonlload(test_data_path_blank_second)\n",
        "\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data_blank_second, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_kelec, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = ElectraBaseClassifier_Cate_Base(len(tf_id_to_name), len(tokenizer_kelec))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_origin, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form_kelec(tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data_blank_second))\n",
        "    df_pred_third = pd.DataFrame(pred_data)\n",
        "\n",
        "    with open('Blank3.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred_third) ):\n",
        "            if len(df_pred_third['annotation'][i]) == 0 :\n",
        "                tmp = '[[\"제품 전체#일반\", [null, 0, 0], \"positive\"]]'\n",
        "                file.write(  '{'+'\\\"id\\\": \\\"{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                    .format( df_pred_third['id'][i]  ,   df_pred_third['sentence_form'][i], tmp ) +'}' ) \n",
        "                file.write(\"\\n\")\n",
        "    \n",
        "    print(\"K Dr!!!\")\n",
        "\n",
        "    test_data_blank_third = jsonlload(test_data_path_blank_third)\n",
        "\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data_blank_third, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_kelec, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = ElectraBaseClassifier_Cate_dr05(len(tf_id_to_name), len(tokenizer_kelec))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_k, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form_kelec(tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data_blank_third))\n",
        "    df_pred_fourth = pd.DataFrame(pred_data)\n",
        "\n",
        "    with open('Blank4.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred_fourth) ):\n",
        "            if len(df_pred_fourth['annotation'][i]) == 0 :\n",
        "                tmp = '[[\"제품 전체#일반\", [null, 0, 0], \"positive\"]]'\n",
        "                file.write(  '{'+'\\\"id\\\": \\\"{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                    .format( df_pred_fourth['id'][i]  ,   df_pred_fourth['sentence_form'][i], tmp ) +'}' ) \n",
        "                file.write(\"\\n\")\n",
        "\n",
        "    print(\"Gpu LR!!\")\n",
        "\n",
        "    test_data_blank_fourth = jsonlload(test_data_path_blank_fourth)\n",
        "\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data_blank_fourth, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_kelec, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = ElectraBaseClassifier_Cate_Base(len(tf_id_to_name), len(tokenizer_kelec))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_gpu_lr, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form_kelec(tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data_blank_fourth))\n",
        "    df_pred_fifth = pd.DataFrame(pred_data)\n",
        "\n",
        "    with open('Blank5.jsonl', 'w') as file:\n",
        "        for i in range( len(df_pred_fifth) ):\n",
        "            if len(df_pred_fifth['annotation'][i]) == 0 :\n",
        "                tmp = '[[\"제품 전체#일반\", [null, 0, 0], \"positive\"]]'\n",
        "                file.write(  '{'+'\\\"id\\\": \\\"{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                    .format( df_pred_fifth['id'][i] ,   df_pred_fifth['sentence_form'][i], tmp ) +'}' ) \n",
        "                file.write(\"\\n\")\n",
        "\n",
        "    print(\"The Last Forcing!!\")\n",
        "\n",
        "    test_data_final = jsonlload(test_data_path_blank_fifth)\n",
        "\n",
        "    entity_property_test_data_kelec, polarity_test_data_kelec = get_dataset(test_data_final, tokenizer_kelec, max_len_elec)\n",
        "\n",
        "    entity_property_test_dataloader = DataLoader(entity_property_test_data_kelec, shuffle=True,\n",
        "                                batch_size=batch_size)\n",
        "\n",
        "    polarity_test_dataloader = DataLoader(polarity_test_data_kelec, shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "    \n",
        "    model = ElectraBaseClassifier_Cate_Base(len(tf_id_to_name), len(tokenizer_kelec))\n",
        "    model.load_state_dict(torch.load(test_category_extraction_model_path_gpu_lr, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "            \n",
        "    polarity_model = ElectraBaseClassifier_Pola_Base(len(polarity_id_to_name), len(tokenizer_kelec))\n",
        "    polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n",
        "    polarity_model.to(device)\n",
        "    polarity_model.eval()\n",
        "\n",
        "    pred_data = predict_from_korean_form_kelec_forcing(tokenizer_kelec, model, polarity_model, copy.deepcopy(test_data_final))\n",
        "\n",
        "    df_pred_final = pd.DataFrame(pred_data)\n",
        "\n",
        "    df_final = pd.concat([df_pred_first, df_pred_second, df_pred_third, df_pred_fourth, df_pred_fifth, df_pred_final]).sort_values(by = ['id'], axis = 0).reset_index(drop = True)\n",
        "\n",
        "    with open('/content/drive/MyDrive/Inference_samples.jsonl', 'w') as file:\n",
        "        for i in range( len(df_final) ):\n",
        "            if len(df_final['annotation'][i]) != 0 :\n",
        "                tmp = str(df_final['annotation'][i]).replace(\"\\'\", \"\\\"\").replace('None', 'null')\n",
        "                file.write(  '{'+'\\\"id\\\": \\\"{0}\\\", \\\"sentence_form\\\": \\\"{1}\\\", \\\"annotation\\\": {2}'\\\n",
        "                    .format( df_final['id'][i],   df_final['sentence_form'][i], tmp ) +'}' ) \n",
        "                file.write(\"\\n\")\n",
        "\n",
        "\n",
        "    return pd.DataFrame(jsonlload('/content/drive/MyDrive/Inference_samples.jsonl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGX_ACpw07vL"
      },
      "outputs": [],
      "source": [
        "Win() # Deberta Kup Original Kdr GPU Forcing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jVCYBGD07vL"
      },
      "source": [
        "## Counting num of Blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stnQbhUx07vL"
      },
      "outputs": [],
      "source": [
        "a = pd.DataFrame(jsonlload('/content/sample.jsonl'))\n",
        "count = 0\n",
        "for i in range(len(a)) :\n",
        "    if len(a['annotation'][i]) == 0 :\n",
        "        count += 1\n",
        "\n",
        "count"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ohA76h6X07vF",
        "n3D62sv607vI"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tf27')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d8015098d1dae84219b5f36314149636e9abcaade3eb06915d5127b55dee25b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
